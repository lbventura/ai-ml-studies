# Convolutional Neural Network (CNN) Architecture for Image Colorization

This is a simple CNN architecture for image colorization based on [assignment 2](http://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/assignments/assignment2.pdf) of the Neural Networks and Deep Learning course of the University of Toronto. See more information in the course page [here](http://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/).

The goal is to colorize grayscale images (e.g, given a black and white image, predict the colors in the image). This is not a straightforward problem because the mapping from grayscale to color is not one-to-one. For example, a grayscale image of a red apple could be colorized in many different ways, depending on the color of the apple.

It is insightful to check the model behavior by visualizing the predictions.

| ![Model Prediction at Epoch 0](examples/unet-0-ex1.png) | ![Model Prediction at Epoch 24](examples/unet-24-ex1.png) |
|:---------------------------------------------------------------:|:---------------------------------------------------------------:|

The image on the left (epoch 0) has a much lower color contrast than the image on the right (epoch 24). The model is learning to represent colors as the training progresses. Another example:

| ![Model Prediction at Epoch 0](examples/unet-0-ex2.png) | ![Model Prediction at Epoch 24](examples/unet-24-ex2.png) |
|:---------------------------------------------------------------:|:---------------------------------------------------------------:|

The image on the left is much lighter than the original image. At epoch 24, the image is more colorful and closer to the original image.

## Why are CNNs used for Image Processing and Computer Vision?

The overall architecture of CNNs achieves three things:

1. Because it preserves the symmetries of the input, a CNN has a better inductive bias (and therefore learn image representation more effectively) than, for example, a fully-connected network. Think of this as adjusting the network architecture - that is, the complicated function that takes an image and tell us whether there is a horse on it or not - to the symmetries of the data: the symmetries will constrain the space of possible solutions, which can help the network generalize better.
<!---
Reference for symmetries restricting the shape of a function.
-->
2. The usage of both convolution and pooling layers enables the hierarchical learning of features. Early layers detect simple elements such as edges, textures, and basic shapes. Intermediate layers combine these basic features to recognize more complex structures like corners, contours, and parts of objects. The deeper layers capture high-level semantic information, including entire objects and backgrounds.

3. The generality of features learned in the early layers allows the same CNN architecture to be applied to multiple tasks. For instance, in this assignment, the network is utilized for both image colorization and super-resolution (e.g., enhancing an image's resolution).

To go deeper, we have to understand convolution and pooling:

- Convolutions ensure that each neuron is connected only to a small, localized region of the input image. This local connectivity allows the network to learn and preserve spatial hierarchies of features. Because convolutions are translation equivariant (see note [1] below), the network can recognize the same feature regardless of its position in the image.

- It is the combination of convolution and pooling layers that introduces translational invariance, allowing feature recognition regardless of its position in the image. Pooling layers also help in two ways: 1) identify the sharpest features in the image, and 2) increase the effective receptive field of a layer (e.g., it can connect different parts of the image which might have similar information. E.g., a repeating background).
<!---
Reference for pooling layers identifying sharpest features and increasing the effective receptive field.
-->

[1]: For practical purposes, equivariance means that if the input image is translated by a certain amount, the feature maps generated by convolutional layers are shifted by the same amount and direction. This is very useful because the network learns the features associated with a monkey (a long tail, a cute nose, etc.) regardless of the position of the monkey in the image. As a friend put it, "a monkey translated is still the same monkey." (Draw an example with a translated monkey)
