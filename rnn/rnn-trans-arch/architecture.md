# Important features of the architecture

-- Image of architecture here --

The following classes are used to implement the encoder-decoder architecture:

- The encoder (`GRUEncoder`) is given by
    - An Embedding layer (i.e, a simple lookup table that stores embeddings of a fixed dictionary and size). For example, the input (dim `batch_size x seq_len`) is a sequence of tokens (e.g, one row is `[ 7,  1,  9, 14,  9, 14,  7, 28]`) and the output is a tensor of floats (dim `batch_size x seq_len x hidden_size`).
    - Followed by a GRU cell. This takes each row of the embedding (along the `seq_len` dimension) and feeds it to the GRU. Each step generates an annotation (i.e, the hidden state computed at each step of the input sequence, corresponding to an encoding of each token) and updates the hidden state.
    - The `forward` method then returns the annotations (e.g, a stack of each word annotation) and the final hidden state.

- The simplest decoder, `RNNDecoder`, is similar to the GRU decoder, as it is given by a sequence of Embedding layer, GRUCell and a Linear layer.
    - The `forward` method takes as inputs the actual word as a tensor, `input`, together with the hidden states of the last step of the encoder, `hidden_init` (dim `(batch_size x hidden_size)`). It computes an embedding over the input (i.e, creates an embedded representation of each token) and passes it together with `hidden_init` to the GRUCell, which computes a new hidden state. The concatenation of the hidden states generated at each step of the input sequence is then passed through the linear layer, in order to generate a vector of unnormalized log probabilities.

- The attention decoders are given by a sequence of Embedding layer -> Attention Layer -> GRUCell -> Linear layer. *Uses both the encoder hidden states and the encoder annotations*
        - Just as before, an embedding is computed for each input, however it is not used directly in the GRUCell. Instead, the embedding is fed to the attention layer, together with the encoder annotations, such that a set of context and attention weights are extracted. These are then concatenated along the first dimension and passed as an input to the GRUCell (which has 2x the input dimension as in the previous decoder). The hidden states generated by the GRUCell are then passed to the output Linear layer.
        - The inputs to the attention layer are:
            - The queries correspond to the current embedding;
            - The keys and the values are given by the annotations (e.g, the encoder hidden states for each step of the input sequence)
        - Three variations of the attention decoder are used:
            - `AdditiveAttention`:  A sequence of Linear -> ReLU -> Linear layers (i.e, the attention layer) and a Softmax function. In the `forward` method, one first computes the unnormalized attention by concatenating the queries and the keys and passing it through the attention layer. The attention weights are then computed by applying a Softmax to the unnormalized attention. The context is then the product of the values with the attention weights. See the mathematical formula in page 5 of the aforementioned assignment.
