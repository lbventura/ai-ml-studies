# Important Features of the Architecture

-- Image of architecture here --

The following classes are used to implement the encoder-decoder architecture:

## Encoder (`GRUEncoder`)

- **Components**: A sequence of Embedding layer and GRUCell.
  - **Embedding Layer**: A simple lookup table that stores embeddings of a fixed dictionary and size. For example, the input (dim `batch_size x seq_len`) is a sequence of tokens (e.g., one row is `[7, 1, 9, 14, 9, 14, 7, 28]`) and the output is a tensor of floats (dim `batch_size x seq_len x hidden_size`).
  - **GRU Cell**: Takes each row of the embedding (along the `seq_len` dimension) and feeds it to the GRU. Each step generates an annotation (i.e., the hidden state computed at each step of the input sequence, corresponding to an encoding of each token) and updates the hidden state.
- **Forward Method**: Returns the annotations (e.g., a stack of each word annotation) and the final hidden state.

## RNNDecoder

- **Components**: A sequence of Embedding layer, GRUCell, and a Linear layer. *Uses only the encoder hidden states*.
- **Forward Method**: Takes as inputs the actual word as a tensor, `input`, together with the hidden states of the last step of the encoder, `hidden_init` (dim `(batch_size x hidden_size)`). It computes an embedding over the input and passes it together with `hidden_init` to the GRUCell, which computes a new hidden state. The concatenation of the hidden states generated at each step of the input sequence is then passed through the linear layer to generate a vector of unnormalized log probabilities.

## Attention Decoders

- **Components**: A sequence of Embedding layer -> Attention Layer -> GRUCell -> Linear layer. *Uses both the encoder hidden states and the encoder annotations*.
- **Forward Method**:
  - An embedding is computed for each input, but it is not used directly in the GRUCell. Instead, the embedding is fed to the attention layer, together with the encoder annotations, to extract a set of context and attention weights. These are then concatenated along the first dimension and passed as an input to the GRUCell (which has 2x the input dimension as in the previous decoder). The hidden states generated by the GRUCell are then passed to the output Linear layer.
  - **Inputs to the Attention Layer**:
    - **Queries**: Correspond to the current embedding.
    - **Keys and Values**: Given by the annotations (e.g., the encoder hidden states for each step of the input sequence).

### Variations of the Attention Decoder
Within the attention decoder, there are several possible implementations of the attention layer:

1. **AdditiveAttention**:
     - **Components**: A sequence of Linear -> ReLU -> Linear layers (i.e., the attention layer) and a Softmax function.
     - **Forward Method**: Computes the unnormalized attention by concatenating the queries and the keys and passing it through the attention layer. The attention weights are then computed by applying a Softmax to the unnormalized attention. The context is then the product of the values with the attention weights. See the mathematical formula in page 5 of [assignment 3](http://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/assignments/assignment3.pdf).
