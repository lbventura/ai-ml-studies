# Results
<!---
TODO: 1. Summarize the results of the models in a table and write a short summary of the main conclusions.
2. Add examples of the model predictions.
3. Clarify if the representation generated by the encoder has a fixed length.
-->
## Conceptual Questions

![Architecture at training time. See original image in page 3 of [assignment 3](http://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/assignments/assignment3.pdf)](figures/encoder-decoder.png)

### 1. How will the architecture of Fig.1 perform for long sequences? Why?

It will not be able to learn as well for two reasons:

1. exploding/vanishing gradients;
2. the encoder architecture uses a fixed-length vector to represent the input sequence. For a long sequence, the decoder will therefore not have sufficient information to represent the input sequence (in other words, there is information loss). Furthermore, the input character is too far away;

### 2. What are some of the techniques we can use to improve the performance of the architecture of Fig.1 for long sequences?

1. Reversing the input sequence (see [Learning to Execute](https://arxiv.org/abs/1410.4615));
2. Clipping the gradients;

### 3. What problems may arise when training with teacher forcing? Consider the differences when we switch from training to testing.

Teacher forcing creates an association between the input ground-truth token and the output. This can be a problem if the training
dataset is not representative. For example, suppose that the word "fox" is always preceded during training by the word "brown".
The model would learn this correlation between the words, and during generation, it could output "fox", even though the overall context was the color of a living room (e.g, "I love the brown wall with the painting").

### 4. Can you think of a way to address the issue? Read the paper [Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks](https://arxiv.org/abs/1506.03099) for some ideas.

Introduce an interpolation between using the actual token (as in training above) and the generated token (as in generalization). Put simply, according to a random sample, one either uses the actual token or the generated token.

## GRU vs Linear Attention vs Transformer

The performance of all models is summarized in the table below. We define accuracy as the fraction of words which are correct and the similarity score as a normalized count of how many letters in a word match. These are computed on a set of the first 2000 words. The underscore `s` indicates a small word (less than or equal to 4 characters), `l` a long word (more than 5 characters). The numbers following the model name indicate the hidden size of the model.

| Metric               | GRU RNN 20 | GRU RNN 40 | Linear Attention RNN 20 | SDT Attention RNN 20 | Transformer 20 |
|----------------------|------------|------------|--------------------------|----------------------|----------------------|
| Acc_S               |      0.37      |    0.73        |        0.97                  |        0.65              |    1.0       |
| Acc_L               |      0.08      |    0.25        |        0.67                  |        0.45              |    0.999       |
| Sim_S               |      0.73      |            |            0.99              |            0.89          |        1.0       |
| Sim_L               |      0.46      |            |            0.85              |            0.72          |        0.999       |
| Vowel Acc           |      0.11      |    0.26        |        0.61                  |        0.47              |    0.98       |
| Cons Acc           |       0.12     |     0.31       |         0.74                 |         0.47             |     0.95       |

Main conclusions:

1. (GRU);
2. (Linear Attention);
3. (Transformer).
